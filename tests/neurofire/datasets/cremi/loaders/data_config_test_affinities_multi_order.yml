# Specify the names of the datasets
dataset_names:
  - A
  - B
  - C

# Specify how the data needs to be sliced before feeding to the network.
# We use a 3D sliding window over the dataset to extract patches, which
# are then fed to the network as batches.
slicing_config:
  # Sliding window size
  window_size:
    A: [5, 576, 576]
    B: [5, 576, 576]
    C: [5, 576, 576]
  # Sliding window stride
  stride:
    A: [1, 352, 352]
    B: [1, 352, 352]
    C: [1, 352, 352]
  # Sliding window downsampling ratio. The actual image size along a
  # dimension is the window_size divided by the downsampling ratio.
  # Example:
  #   window_size = [1, 512, 512], downsampling_ratio = [1, 2, 2] ==>
  #   slice shape = [1, 256, 256]
  downsampling_ratio:
    A: [1, 1, 1]
    B: [1, 1, 1]
    C: [1, 1, 1]
  # Reflect padding on the loaded volume. Follows numpy.pad semantics.
  padding:
    A: [[1, 1], [15, 15], [15, 15]]
    B: [[1, 1], [15, 15], [15, 15]]
    C: [[1, 1], [15, 15], [15, 15]]
  # Data slice to iterate over.
  data_slice:
    A: '0:80, :, :'
    B: '0:80, :, :'
    C: '0:80, :, :'

# Specify paths to volumes
volume_config:
  # Raw data
  raw:
    path:
      A: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleA/raw/sampleA_raw_none.h5'
      B: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleB/raw/sampleB_raw_none.h5'
      C: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleC/raw/sampleC_raw_none.h5'
    # CREMI default is '/volumes/raw'
    path_in_h5_dataset:
      A: 'data'
      B: 'data'
      C: 'data'
    # Optionally, we specify training precision
    dtype: float32
  # Membranes
  affinities:
    path:
      A: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleA/gt/sampleA_neurongt_none.h5'
      B: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleB/gt/sampleB_neurongt_none.h5'
      C: '/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi/sampleC/gt/sampleC_neurongt_none.h5'
    # CREMI default is '/volumes/labels/neuron_ids'
    path_in_h5_dataset:
      A: 'data'
      B: 'data'
      C: 'data'
    # Specify training precision
    dtype: float32
    # Dimensionality of the affinity computation
    affinity_dim: 3
    affinity_order: [[1, 1, 1], [2, 4, 4], [3, 8, 8], [4, 16, 16]]
    retain_segmentation: true


# Configuration for the master dataset.
master_config:
  # We might need order 0 interpolation if we have segmentation in there somewhere.
  elastic_transform:
    alpha: 2000.
    sigma: 50.
    order: 0
  crop_after_elastic_transform:
    size: 512
